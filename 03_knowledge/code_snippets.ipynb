{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee4cd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015f800c-30ad-4fc0-abcd-9cb5287041ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Class labels in default model\n",
    "model = YOLO(\"8-epoch-omelette.pt\")\n",
    "results = model.predict(source=\"./manual_test_data/omelette_1.jpeg\")\n",
    "model.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65a7aff-b683-4588-a8f9-630ce749c047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#detection of pre-specified classes\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "results = model.predict(source=\"./manual_test_data/omelette_1.jpeg\", classes=[45, 46, 47, 48, 49, 51, 52, 53, 54, 55], save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2283489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#measuring of time to predict\n",
    "import time\n",
    "\n",
    "model = YOLO(\"yolov8l.pt\")\n",
    "time_start = time.perf_counter()\n",
    "results = model.predict(source=\"./manual_test_data/omelette_1.jpeg\")\n",
    "time_elapsed = (time.perf_counter() - time_start)\n",
    "print(\"The task took\", round(time_elapsed, 2), \" seconds\")\n",
    "\n",
    "#results in seconds (local machine)\n",
    "#nano: 0.46\n",
    "#small: 0.87\n",
    "#medium: 1.82\n",
    "#large: 2.84\n",
    "#xl: 4.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd4e395",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation of model\n",
    "model.val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0f4d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#loading pre-trained model\n",
    "#model = YOLO(\"path/to/model.pt\")\n",
    "model = YOLO(\"8-epoch-omelette.pt\")\n",
    "\n",
    "#make prediction\n",
    "#param source = image to predict contents of\n",
    "#param conf = confidence threshold\n",
    "#param classes = classes included in search (9 food items and bowls)\n",
    "#results = model.predict(source=\"./test_images/pizza.jpeg\", conf=0.25, classes=[45, 46, 47, 48, 49, 51, 52, 53, 54, 55], save=True)\n",
    "results = model.predict(source=\"./manual_test_data/omelette_3.jpeg\", save=True)\n",
    "\n",
    "#BRANCH 1: plate is detected - ration between food and plate\n",
    "if any(results[0].boxes.cls == 45):\n",
    "    #determine size of bowl (class 45)\n",
    "    #box = results[0].boxes[results[0].boxes.cls == 45].xywh\n",
    "    print(1)\n",
    "    \n",
    "#BRANCH 2: plate is not detected - size of food (assuming standard distance from food)\n",
    "elif all(results[0].boxes.cls != 45):\n",
    "    \n",
    "    #calculate normalized width and height\n",
    "    pos_tensor = results[0].boxes.xywhn\n",
    "    \n",
    "    #convert to numpy array\n",
    "    pos_numpy = pos_tensor.detach().numpy()\n",
    "    print(pos_numpy)\n",
    "    \n",
    "\n",
    "    #get min height / width for each object\n",
    "    object_wh = np.delete(pos_numpy, [0,1], 1)\n",
    "    object_height = (object_wh.min(axis=1)) / 2\n",
    "    print(object_height)\n",
    "    \n",
    "    #calculate object area\n",
    "    width_array = pos_numpy[:,2]\n",
    "    height_array = pos_numpy[:,3]\n",
    "    object_area = width_array * height_array \n",
    "    print (object_area)\n",
    "    \n",
    "    #calculate object volume\n",
    "    factor_scaling = 0.75\n",
    "    object_volume = object_area * object_height * factor_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd724527",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "#mask size detection\n",
    "mask_object = results[0].masks # get Mask object\n",
    "mask_shape = mask_object.shape\n",
    "mask_segment = mask_object.segments # Bounding coordinates of masks -> List[segment] * N\n",
    "mask_data = mask_object.data # raw masks tensor, (N, H, W)\n",
    "# Iterate through the elements in tensor\n",
    "for i in range(mask_data.shape[0]):\n",
    "    zeros = (mask_data[i] == 0).sum()\n",
    "    ones = (mask_data[i] == 1).sum()\n",
    "    area_covered = ones / (zeros+ones) # Share of mask\n",
    "    print ('Masks Raw. Mask No:', i, 'Class', results[0].boxes[i].cls, 'Area Covered:', area_covered, 'Shape:', mask_data[i].shape)\n",
    "for i in range(len(mask_segment)):\n",
    "    print ('Segment: Pixels in boundary:', len(mask_segment[i])) # No. of pixels in the boundary\n",
    "print ('Pixels in Picture', int(mask_data.shape[1]) * int(mask_data.shape[2])) # No. of pixels in picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1d6f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Yolo training \n",
    "### Part 1 - yaml file with the following structure and information\n",
    "\n",
    "path: ./food-data.yaml/\n",
    "train: 'train/images'\n",
    "val: 'valid/images'\n",
    " \n",
    "# class names\n",
    "names: \n",
    "  0: 'apple', 1: 'banana', 2: ...\n",
    "  \n",
    "### Part 2 - folder structure\n",
    "├── data\n",
    "## └── train\n",
    "####└── images (folder including all training images)\n",
    "####└── labels (folder including all training labels)\n",
    "## └── test\n",
    "####└── images (folder including all testing images)\n",
    "####└── labels (folder including all testing labels)\n",
    "## └── valid\n",
    "####└── images (folder including all valid images)\n",
    "####└── labels (folder including all valid labels)\n",
    "\n",
    "\n",
    "### Part 3 - txt file with following data\n",
    "Labels: Class label, X_Center, Y_Center, Width, Height\n",
    "Example: [0 0.484167 0.222500 0.275000 0.351667]\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m103"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "f90c6fdffd3857f59eaf8bbf91c717032204bda658957ce3e390c08fe4c07aee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
